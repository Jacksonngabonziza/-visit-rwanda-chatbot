{
  "Sim-card": {
    "precision": 1.0,
    "recall": 0.7272727272727273,
    "f1-score": 0.8421052631578948,
    "support": 11,
    "confused_with": {
      "out_of_scope": 1,
      "Transportation": 1
    }
  },
  "Shopping_malls": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 8,
    "confused_with": {}
  },
  "out_of_scope": {
    "precision": 0.68,
    "recall": 0.7391304347826086,
    "f1-score": 0.7083333333333334,
    "support": 23,
    "confused_with": {
      "Transportation": 4,
      "deny": 1
    }
  },
  "Exchange_money": {
    "precision": 0.8181818181818182,
    "recall": 0.8181818181818182,
    "f1-score": 0.8181818181818182,
    "support": 11,
    "confused_with": {
      "out_of_scope": 1,
      "goodbye": 1
    }
  },
  "bot_challenge": {
    "precision": 0.7647058823529411,
    "recall": 1.0,
    "f1-score": 0.8666666666666666,
    "support": 26,
    "confused_with": {}
  },
  "greet": {
    "precision": 0.7777777777777778,
    "recall": 0.7241379310344828,
    "f1-score": 0.75,
    "support": 29,
    "confused_with": {
      "goodbye": 3,
      "deny": 3
    }
  },
  "goodbye": {
    "precision": 0.8148148148148148,
    "recall": 0.6285714285714286,
    "f1-score": 0.7096774193548386,
    "support": 35,
    "confused_with": {
      "deny": 6,
      "greet": 4
    }
  },
  "Sites_to_visit": {
    "precision": 0.75,
    "recall": 0.8571428571428571,
    "f1-score": 0.7999999999999999,
    "support": 7,
    "confused_with": {
      "Transportation": 1
    }
  },
  "deny": {
    "precision": 0.75,
    "recall": 0.7692307692307693,
    "f1-score": 0.7594936708860761,
    "support": 39,
    "confused_with": {
      "out_of_scope": 3,
      "bot_challenge": 3
    }
  },
  "Order_food": {
    "precision": 1.0,
    "recall": 0.8181818181818182,
    "f1-score": 0.9,
    "support": 11,
    "confused_with": {
      "Transportation": 1,
      "Sites_to_visit": 1
    }
  },
  "Transportation": {
    "precision": 0.5333333333333333,
    "recall": 0.6666666666666666,
    "f1-score": 0.5925925925925926,
    "support": 12,
    "confused_with": {
      "out_of_scope": 2,
      "bot_challenge": 1
    }
  },
  "accuracy": 0.7735849056603774,
  "macro avg": {
    "precision": 0.8080739660418804,
    "recall": 0.7953196773695613,
    "f1-score": 0.7951864331066564,
    "support": 212
  },
  "weighted avg": {
    "precision": 0.7853605047889176,
    "recall": 0.7735849056603774,
    "f1-score": 0.773152037018039,
    "support": 212
  }
}